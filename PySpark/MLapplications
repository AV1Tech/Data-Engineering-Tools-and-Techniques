To use PySpark for SQL operations and machine learning (ML) applications, you'll leverage the powerful capabilities of Apache Spark through its Python API, PySpark. Below are the basic steps and examples for both SQL operations and ML applications using PySpark:

### Using PySpark for SQL Operations

PySpark provides a SQL module called `pyspark.sql` that allows you to perform SQL queries on structured data, similar to using SQL with a traditional database.

#### Example: SQL Operations

```python
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PySpark SQL Example") \
    .getOrCreate()

# Create a DataFrame from a JSON file
df = spark.read.json("path/to/json_file.json")

# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")

# Perform SQL queries
sql_result = spark.sql("SELECT * FROM people WHERE age >= 18")

# Show the result
sql_result.show()

# Stop SparkSession
spark.stop()
```

### Using PySpark for Machine Learning Applications

PySpark's MLlib library provides scalable machine learning algorithms and utilities. It integrates seamlessly with PySpark's DataFrame API, allowing you to preprocess data, train models, and make predictions.

#### Example: ML Applications

```python
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PySpark ML Example") \
    .getOrCreate()

# Load dataset
df = spark.read.csv("path/to/dataset.csv", header=True, inferSchema=True)

# Prepare features and label
feature_cols = df.columns[:-1]  # assuming the last column is the target
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
df = assembler.transform(df).select("features", df.columns[-1].alias("label"))

# Split data into training and testing sets
train_data, test_data = df.randomSplit([0.8, 0.2])

# Initialize and train a model
lr = LinearRegression()
model = lr.fit(train_data)

# Make predictions
predictions = model.transform(test_data)

# Evaluate the model
from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(predictions)
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Stop SparkSession
spark.stop()
```

### Explanation:

- **Initialization:** `SparkSession` is initialized to create a connection to Spark.
- **SQL Operations:** 
  - Data is read into a DataFrame (`df`), which can be queried using SQL.
  - `createOrReplaceTempView` registers the DataFrame as a temporary SQL view (`people`).
  - SQL queries are executed using `spark.sql()` method.
- **Machine Learning:**
  - `VectorAssembler` is used to assemble feature columns into a vector column.
  - Data is split into training and testing sets.
  - An ML model (in this case, `LinearRegression`) is initialized, trained, and used to make predictions.
  - Model evaluation is done using `RegressionEvaluator`.
  
### Notes:

- Ensure that `SPARK_HOME` environment variable is properly set.
- Adjust file paths (`"path/to/json_file.json"` and `"path/to/dataset.csv"`) according to your local setup.
- SparkSession (`spark`) should be stopped using `spark.stop()` to release resources after use.

By following these examples, you can effectively use PySpark for both SQL operations on structured data and building machine learning applications at scale. Adjust and expand these examples based on your specific data and requirements.